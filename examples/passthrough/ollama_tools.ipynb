{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.render import render_text_description # to describe tools as a string \n",
    "from langchain_core.tools import tool # tools for our llm\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tools_logger = logging.getLogger(\"tools\")\n",
    "@tool\n",
    "def accept(reason) -> int:\n",
    "    \"Accept a reason and return 1.\"\n",
    "    tools_logger.info(f\"Accepting reason: {reason}\")\n",
    "    return 1\n",
    "\n",
    "\n",
    "@tool\n",
    "def reject(reason) -> int:\n",
    "    \"Reject a reason and return 0.\"\n",
    "    tools_logger.info(f\"Rejecting reason: {reason}\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accept: accept(reason) -> int - Accept a reason and return 1.\n",
      "reject: reject(reason) -> int - Reject a reason and return 0.\n"
     ]
    }
   ],
   "source": [
    "tools = [accept,reject]\n",
    "rendered_tools = render_text_description(tools)\n",
    "print(rendered_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant that has access to functions.\n",
      "ONLY Use them:\n",
      "\n",
      "accept: accept(reason) -> int - Accept a reason and return 1.\n",
      "reject: reject(reason) -> int - Reject a reason and return 0.\n",
      "\n",
      "\n",
      "Given the user input, return the name and input of the tool to use.\n",
      "Return your response as a JSON blob with 'name' and 'arguments' keys.\n",
      "The value associated with the 'arguments' key should be a dictionary of parameters.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = f\"\"\"You are an assistant that has access to functions.\n",
    "ONLY Use them:\n",
    "\n",
    "{rendered_tools}\n",
    "\n",
    "\n",
    "Given the user input, return the name and input of the tool to use.\n",
    "Return your response as a JSON blob with 'name' and 'arguments' keys.\n",
    "The value associated with the 'arguments' key should be a dictionary of parameters.\"\"\"\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "def LLM(prompt, system_message, temperature, max_tokens,top_p, model,return_json=False):\n",
    "    system_message = \"Todays DateTime:\"+datetime.now().strftime(\"%Y-%m-%d %H:00:00\")+\"\\n\"+system_message\n",
    "    url = 'http://localhost:11435/api/passthrough/chat'\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    payload = {\n",
    "        'model': model,\n",
    "        'cache': '1h',\n",
    "        'keep_alive': '0m',\n",
    "        'messages': [\n",
    "            {'role': 'system', 'content': system_message},\n",
    "            {'role': 'user', 'content': prompt}\n",
    "        ],\n",
    "        'temperature': temperature,\n",
    "        'top_p': top_p,\n",
    "        'max_tokens': max_tokens,\n",
    "        'return_json': return_json\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        response.raise_for_status()  # Raise an HTTPError if the HTTP request returned an unsuccessful status code\n",
    "        data = response.json()\n",
    "        #print('Success:', data)\n",
    "        return data\n",
    "    except requests.exceptions.RequestException as error:\n",
    "        print('Error:', error)\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3-groq-tool-use',\n",
       " 'created_at': '2024-07-22T19:58:47.235141467Z',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': '{\"name\":\"accept\", \"arguments\": {\"reason\": \"You are being traded: 100GP, You are acquiring: Cow (10gp)\"}}'},\n",
       " 'done': True,\n",
       " 'total_duration': 25687984353,\n",
       " 'load_duration': 25090615053,\n",
       " 'prompt_eval_count': 175,\n",
       " 'prompt_eval_duration': 88548000,\n",
       " 'eval_count': 32,\n",
       " 'eval_duration': 494865000}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLM(\"\"\"\n",
    "    You are a video game bot.\n",
    "    I am being traded:\n",
    "    100GP\n",
    "    I am aquiring:\n",
    "    Cow (10gp)\n",
    "    Only Use the provided tools.\n",
    "    \n",
    "    \n",
    "    \n",
    "   \"\"\",system_prompt,temperature=0.5, top_p=0.65,model=\"llama3-groq-tool-use\",max_tokens=9999,return_json=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3-groq-tool-use',\n",
       " 'created_at': '2024-07-22T19:41:03.906080529Z',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': \"I'm sorry but I don't have the capability to fetch real-time weather information. However, if you need help with anything else, feel free to ask!\"},\n",
       " 'done': True,\n",
       " 'total_duration': 535583327,\n",
       " 'load_duration': 569981,\n",
       " 'prompt_eval_count': 245,\n",
       " 'prompt_eval_duration': 91538000,\n",
       " 'eval_count': 33,\n",
       " 'eval_duration': 436435000}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLM(\"\"\"\n",
    "    what is the weather in San Francisco?\n",
    "    \"\"\",\"\"\"\n",
    "    \n",
    "    \n",
    "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
    "<tool_call>\n",
    "{\"name\": <function-name>,\"arguments\": <args-dict>}\n",
    "</tool_call>\n",
    "\n",
    "Here are the available tools:\n",
    "<tools> {\n",
    "    \"name\": \"get_current_weather\",\n",
    "    \"description\": \"Get the current weather in a given location\",\n",
    "    \"parameters\": {\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"unit\": {\n",
    "                \"enum\": [\n",
    "                    \"celsius\",\n",
    "                    \"fahrenheit\"\n",
    "                ],\n",
    "                \"type\": \"string\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\n",
    "            \"location\"\n",
    "        ],\n",
    "        \"type\": \"object\"\n",
    "    }\n",
    "} </tools>\n",
    "\n",
    "\"\"\",temperature=0.5, top_p=0.65 ,model=\"llama3-groq-tool-use\",max_tokens=9999,return_json=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
